{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#create a class in which acts as a base for different algorithms. \n",
    "#specifically the train and predict steps of different models. \n",
    "#inputdata = the input data \n",
    "#output = the output and not  really used in unsupervised alogorithms \n",
    "#**kwargs lets it use parameters without having to state them \n",
    "class MachineLearningALgos:\n",
    "    def train(self, inputdata, output, **kwargs):\n",
    "        raise NotImplementedError(\"Check subclass for training.\")\n",
    "    def predict(self, inputdata):\n",
    "        raise NotImplementedError(\"Check subclass for predicting.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this class is responsible for kmeans clustering unsupervised\n",
    "#klusters = the amount of clusters for the kmeans \n",
    "#maxiterations = is the amount of times the computer can reiterate through\n",
    "#the code until it automatically stops \n",
    "#conver = when finidng centroids, and the difference between the last\n",
    "#centroid is lower than this threshold, finding centroids stop. \n",
    "#cent = the centroids, rn we have none\n",
    "class kmclus(MachineLearningALgos):\n",
    "    def __init__(self, klusters, maxiterations=100, conver=1e-4):\n",
    "        self.klusters = klusters\n",
    "        self.maxiterations = maxiterations\n",
    "        self.conver = conver\n",
    "        self.cent = None\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    #this is a definition for finding the centroids in the data\n",
    "    #isitbool sees if the data is priperly labeled \n",
    "    #goes through each of the clusters and find centroids \n",
    "    #makes sure there are outputs and data lables \n",
    "    #cluspoint is the data point of cluster, and only current cluster \n",
    "    #info storeed when at it. then centroid is stored\n",
    "    def findthecent(self, inputdata, isitbool=None, output=None):\n",
    "        self.cent = np.zeros((self.klusters, inputdata.shape[1]))\n",
    "        for klus in range(self.klusters):\n",
    "            if isitbool is not None and output is not None:\n",
    "                cluspoint = inputdata[isitbool & (output == klus)]\n",
    "                if len(cluspoint) > 0:\n",
    "                    self.cent[klus] = np.mean(cluspoint, axis=0)\n",
    "                    print(f\"randomly assigning centroid for cluster {klus}\")\n",
    "                    continue\n",
    "            #use np random in case there is no data labels and it will be assigned randomly\n",
    "            self.cent[klus] = inputdata[np.random.choice(inputdata.shape[0])]\n",
    "        print(\"found the centroids\")\n",
    "        \n",
    "    \n",
    "    #make clusters based on the centroids we found in findthecent\n",
    "    #dist = the euc distance between centroids and datapoints on normalizaed data\n",
    "    #calculatiosn done with help of numpy \n",
    "    def theclusters(self, inputdata):\n",
    "        #make a place for the cluster assignments to go\n",
    "        assklus= []\n",
    "        for pointofdata in inputdata:\n",
    "            mindist = float('inf')\n",
    "            nearbycent = None\n",
    "            for i, centroid in enumerate(self.cent):\n",
    "                dist=np.linalg.norm(pointofdata - centroid)\n",
    "                if dist < mindist:\n",
    "                    mindist = dist\n",
    "                    nearbycent = i \n",
    "            assklus.append(nearbycent)\n",
    "        return np.array(assklus)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #lbls = stores where all the data points were assigned to includes the index of the cluster \n",
    "    #finds newer centroids of each cluster by finding the mean of all of its data points\n",
    "    #stores them in newcent, and if there is no info then it is done randomly \n",
    "    def calcthecurrcent(self, inputdata, lbls):\n",
    "        newcent= np.zeros((self.klusters, inputdata.shape[1]))\n",
    "        for klus in range(self.klusters):\n",
    "            cluspoint = inputdata[lbls == klus]\n",
    "            if len(cluspoint) > 0:\n",
    "                centsum = np.zeros(inputdata.shape[1])\n",
    "                for dpoint in cluspoint:\n",
    "                    centsum += dpoint\n",
    "                newcent[klus] = centsum / len(cluspoint)\n",
    "            else:\n",
    "                print(\"error, no info must be done randomly.\")\n",
    "                newcent[klus] = inputdata[np.random.choice(inputdata.shape[0])]\n",
    "        print(f\"the new centroids are {newcent}\")\n",
    "        return newcent\n",
    "    \n",
    "    \n",
    "    #checks if reacked the minimum amount on convergence to stop initializing the centroids. \n",
    "    #if less than the defined then it stops\n",
    "    #it also stops it once the max iterations occurs. this is applied to bot the kmeans algos\n",
    "    #it starts off as true because it is implies that it is \n",
    "    def isitconverged(self, oldcent, newcent, iteri):\n",
    "        for i in range(len(oldcent)):\n",
    "            #find euc dist \n",
    "            itdiffers= np.sqrt(np.sum((newcent[i]-oldcent[i])**2))\n",
    "            if itdiffers >= self.conver:\n",
    "                return False #false means it is NOT converged \n",
    "        if iteri >= self.maxiterations -1:\n",
    "            print(\"max iterations completed stopping iterations now.\")\n",
    "            return True #max iterations ends before convergence, convergence can keep going \n",
    "        return True\n",
    "        \n",
    "        \n",
    "\n",
    "    #now we train the mdoel definition , make sure its numpy array and make sure hparamaters make sense\n",
    "    def train(self, inputdata, output=None):\n",
    "        inputdata = np.array(inputdata)\n",
    "        if not (self.klusters > 0 and isinstance(self.klusters, int)):\n",
    "            raise ValueError(\"has to be a postive number\")\n",
    "        if not (self.conver >= 0):\n",
    "            raise ValueError(\"cant be a negative number\")\n",
    "        if not (self.maxiterations > 0 and isinstance(self.maxiterations, int)):\n",
    "            raise ValueError(\"has to be a postive number\")\n",
    "        \n",
    "        #now we randomize the starting clusters \n",
    "        print(\"random clusters are being assigned right now\")\n",
    "        iniklus = np.random.choice(range(self.klusters), size=inputdata.shape[0])\n",
    "        inputdata = np.hstack([inputdata, iniklus[:, None]])\n",
    "        feats = inputdata.shape[1]-1\n",
    "        \n",
    "        #find centroids until converegence or max iteraitons \n",
    "        #keps going until it finds the best centroid\n",
    "        self.cent = np.zeros((self.klusters, feats))\n",
    "        lbls = iniklus\n",
    "        for iteri in range(self.maxiterations):\n",
    "            print(f\"\\n on iteration number {iteri + 1}\")\n",
    "            #assigning clusters\n",
    "            lbls = self.theclusters(inputdata[:,:-1])\n",
    "            print(f\"on iteration {iteri + 1} and the assignments are {lbls}\")\n",
    "            #calculating the centroids \n",
    "            newcent = self.calcthecurrcent(inputdata[:,:-1], lbls)\n",
    "            print(f\"on iteration {iteri + 1}and the new centroids are {newcent}\")\n",
    "\n",
    "            #checks convergence ansd max iterations and if its done it stops. \n",
    "            if self.isitconverged(self.cent, newcent, iteri):\n",
    "                print(\"converged or max iterations, stopping now.\")\n",
    "                break\n",
    "            self.cent = newcent\n",
    "        return self.cent, lbls\n",
    "#now prediction happens and the clusters datapoints fall into are predicted. \n",
    "#predicted with the clusters def and input data is given. \n",
    "#semi sup usees this predict too \n",
    "\n",
    "    def predict(self, inputdata):\n",
    "        if self.cent is None:\n",
    "            raise ValueError(\"train model before prediction\")\n",
    "        return self.theclusters(np.array(inputdata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is semisupervised version of kmeans and this can handle null data.\n",
    "class semikmclus(kmclus):\n",
    "    def train(self, inputdata, output=None):\n",
    "        if output is None:\n",
    "            raise ValueError(\"data must be labeled for semi supervised kmeans. can be null.\")\n",
    "        \n",
    "        #input and output data are arrays\n",
    "        inputdata = np.array(inputdata)\n",
    "        output = np.array(output)\n",
    "        \n",
    "        #do this again : make sure hparamaters make sense\n",
    "        if not (self.klusters > 0 and isinstance(self.klusters, int)):\n",
    "            raise ValueError(\"has to be a postive number\")\n",
    "        if not (self.conver >= 0):\n",
    "            raise ValueError(\"cant be a negative number\")\n",
    "        if not (self.maxiterations > 0 and isinstance(self.maxiterations, int)):\n",
    "            raise ValueError(\"has to be a postive number\")\n",
    "        \n",
    "        #make sure data is labeled with boolean true or false \n",
    "        isitbool = ~np.isnan(output)\n",
    "        itsnotbool = np.isnan(output)\n",
    "        \n",
    "        #find centroids\n",
    "        self.findthecent(inputdata[:,:-1], isitbool=isitbool, output=output)\n",
    "        \n",
    "        #assign clusters \n",
    "        clusassign = np.zeros(inputdata.shape[0], dtype=int)\n",
    "        clusassign[isitbool] = output[isitbool].astype(int)\n",
    "        \n",
    "        #do this again : find centroifds \n",
    "        for iteri in range(self.maxiterations):\n",
    "            print(f\"\\n on iteration number {iteri + 1}\")\n",
    "            \n",
    "            #need to remove the last column \n",
    "            updateddata=inputdata[:,:-1] \n",
    "            \n",
    "            #unlabeled data are assigned to clusters \n",
    "            clusassign[itsnotbool] = self.theclusters(updateddata[itsnotbool])\n",
    "            print(f\"cluster assignments are {clusassign}\")\n",
    "            \n",
    "            #new centroids found \n",
    "            newcent = self.calcthecurrcent(updateddata, clusassign)\n",
    "            print(f\"newest centroids are \\n{newcent}\")\n",
    "            \n",
    "            #see if data is convereged or hit max iteratiosn \n",
    "            if self.isitconverged(self.cent, newcent, iteri):\n",
    "                print(\"converged or max iterations, stopping now.\")\n",
    "                break\n",
    "            self.cent = newcent\n",
    "        return self.cent, clusassign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#question 3 \n",
    "\n",
    "#do PCA \n",
    "#numcom = number of components \n",
    "#pcom is the principle components \n",
    "class princompanaly(MachineLearningALgos):\n",
    "    def __init__(self, numcom):\n",
    "        self.numcom = numcom\n",
    "        self.meanz = None\n",
    "        self.standev = None\n",
    "        self.pcom = None\n",
    "#edit the data normalizing it and standard dev and mean (standardizing) \n",
    "    def standdata(self, inputdata):\n",
    "        print(\"standardize the data before doing pca\")\n",
    "        self.meanz = np.mean(inputdata, axis=0)\n",
    "        self.standev = np.std(inputdata, axis=0)\n",
    "        return (inputdata - self.meanz) / self.standev\n",
    "#this definition is responsible for undoing the standarization of the dataset after PCA\n",
    "#standdataset is the dataset that is standardized \n",
    "    def notstanddata(self, standdataset):\n",
    "        print(\"making dataset back to original (removing standardizatiosn)\")\n",
    "        return (standdataset * self.standev) + self.meanz\n",
    "#now we train the PCA model \n",
    "#input data and labels are defined but labels are not used in PCA \n",
    "#make sure the number of components and size is valid \n",
    "    def train(self, inputdata, output=None):\n",
    "        if not (self.numcom > 0 and self.numcom <= min(inputdata.shape)):\n",
    "            raise ValueError(\"number of componenets is either neg or not right shape. recheck.\")\n",
    "#use the standdata def to standardize and define the dataset for pca\n",
    "        standdataset = self.standdata(inputdata)\n",
    "#now do svd \n",
    "        print(\"starting single value decomp\")\n",
    "#now we have the vecorts but they arent used in pca \n",
    "#we have the single values which is sqrt of eignvalues \n",
    "#we also have the transposed vecors \n",
    "#full matricies is in numpy to say that matriz is going to be reduced \n",
    "        svdvec, singval, transpvec = np.linalg.svd(standdataset, full_matrices=False)\n",
    "#now we do matrix verification \n",
    "        covmatx = np.cov(standdataset, rowvar=False)\n",
    "        eignvec = transpvec.T\n",
    "        appcov = eignvec @ np.diag(singval**2 / (standdataset.shape[0] - 1)) @ eignvec.T\n",
    "#check if tansposed vec have the eighn\n",
    "        if np.allclose(covmatx, appcov, atol=1e-6):\n",
    "            print(\"eighnvecors are in the matrix\")\n",
    "        else:\n",
    "            raise ValueError(\"eighnvectors aare not in the maxrix check whats going on\")\n",
    "#numcom defined says how many pc's there are going to be and picks the top ones for the amount deifned \n",
    "        print(f\"based on numcom there are {self.numcom} pc(s) being picked right now\")\n",
    "        self.pcom = transpvec[:self.numcom]\n",
    "#now data is transformed with pca on input data \n",
    "    def predict(self, inputdata):\n",
    "        if self.pcom is None:\n",
    "            raise ValueError(\"model has to be trained before predicting\")\n",
    "#use the standdata def to standardize and define the dataset for pca\n",
    "        standdataset = self.standdata(inputdata)\n",
    "#compress for pca transformation \n",
    "        print(\"data is compressing\")\n",
    "        compresseddata = self.pcom @ standdataset.T\n",
    "#standardize and decompress the data \n",
    "        print(\"data being rebuilt\")\n",
    "        decompstandata = self.pcom.T @ compresseddata\n",
    "#unstandardize the data back to original value scale \n",
    "        datadecompfin = self.notstanddata(decompstandata.T)\n",
    "        return datadecompfin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " now doing unsupervised k means clustering\n",
      "random clusters are being assigned right now\n",
      "\n",
      " on iteration number 1\n",
      "on iteration 1 and the assignments are [0 0 0 0 0 0]\n",
      "error, no info must be done randomly.\n",
      "the new centroids are [[ 4.25        5.23333333]\n",
      " [ 9.         11.        ]]\n",
      "on iteration 1and the new centroids are [[ 4.25        5.23333333]\n",
      " [ 9.         11.        ]]\n",
      "\n",
      " on iteration number 2\n",
      "on iteration 2 and the assignments are [0 0 0 1 0 1]\n",
      "the new centroids are [[2.125 3.1  ]\n",
      " [8.5   9.5  ]]\n",
      "on iteration 2and the new centroids are [[2.125 3.1  ]\n",
      " [8.5   9.5  ]]\n",
      "\n",
      " on iteration number 3\n",
      "on iteration 3 and the assignments are [0 0 1 1 0 1]\n",
      "the new centroids are [[1.16666667 1.46666667]\n",
      " [7.33333333 9.        ]]\n",
      "on iteration 3and the new centroids are [[1.16666667 1.46666667]\n",
      " [7.33333333 9.        ]]\n",
      "\n",
      " on iteration number 4\n",
      "on iteration 4 and the assignments are [0 0 1 1 0 1]\n",
      "the new centroids are [[1.16666667 1.46666667]\n",
      " [7.33333333 9.        ]]\n",
      "on iteration 4and the new centroids are [[1.16666667 1.46666667]\n",
      " [7.33333333 9.        ]]\n",
      "converged or max iterations, stopping now.\n",
      "the centroids in the data are \n",
      " [[1.16666667 1.46666667]\n",
      " [7.33333333 9.        ]]\n",
      "the datapoints are assigned to these clusters \n",
      " [0 0 1 1 0 1]\n",
      "the new clusters are predicted to be assign to \n",
      " [0 1]\n",
      "\n",
      " now doing semi supervised k means clustering\n",
      "randomly assigning centroid for cluster 0\n",
      "found the centroids\n",
      "\n",
      " on iteration number 1\n",
      "cluster assignments are [0 0 0 1 0 1]\n",
      "the new centroids are [[2.125]\n",
      " [8.5  ]]\n",
      "newest centroids are \n",
      "[[2.125]\n",
      " [8.5  ]]\n",
      "\n",
      " on iteration number 2\n",
      "cluster assignments are [0 0 0 1 0 1]\n",
      "the new centroids are [[2.125]\n",
      " [8.5  ]]\n",
      "newest centroids are \n",
      "[[2.125]\n",
      " [8.5  ]]\n",
      "converged or max iterations, stopping now.\n",
      "the centroids in the data are \n",
      " [[2.125]\n",
      " [8.5  ]]\n",
      "the datapoints are assigned to these clusters \n",
      " [0 0 0 1 0 1]\n",
      "\n",
      " now doing pca\n",
      "standardize the data before doing pca\n",
      "starting single value decomp\n",
      "eighnvecors are in the matrix\n",
      "based on numcom there are 2 pc(s) being picked right now\n",
      "the original data \n",
      " [[ 1.   2. ]\n",
      " [ 1.5  1.8]\n",
      " [ 5.   8. ]\n",
      " [ 8.   8. ]\n",
      " [ 1.   0.6]\n",
      " [ 9.  11. ]]\n",
      "standardize the data before doing pca\n",
      "data is compressing\n",
      "data being rebuilt\n",
      "making dataset back to original (removing standardizatiosn)\n",
      "data after the pca \n",
      " [[ 1.   2. ]\n",
      " [ 1.5  1.8]\n",
      " [ 5.   8. ]\n",
      " [ 8.   8. ]\n",
      " [ 1.   0.6]\n",
      " [ 9.  11. ]]\n"
     ]
    }
   ],
   "source": [
    "#we use main to run the code and define the input data \n",
    "def main():\n",
    "#now doing unsup kmean clustering\n",
    "    print(\"\\n now doing unsupervised k means clustering\")\n",
    "    inputdata = np.array([\n",
    "        [1.0, 2.0],\n",
    "        [1.5, 1.8],\n",
    "        [5.0, 8.0],\n",
    "        [8.0, 8.0],\n",
    "        [1.0, 0.6],\n",
    "        [9.0, 11.0]\n",
    "    ])\n",
    "    kmeanz = kmclus(klusters=2, maxiterations=100, conver=1e-4)\n",
    "    cent, labelz = kmeanz.train(inputdata)\n",
    "    print(\"the centroids in the data are \\n\", cent)\n",
    "    print(\"the datapoints are assigned to these clusters \\n\", labelz)\n",
    "#use predict to find what clusters belong to which new data  \n",
    "    newdataaaa = np.array([[0.5, 2.0], [6.0, 8.0]])\n",
    "    predis = kmeanz.predict(newdataaaa)\n",
    "    print(\"the new clusters are predicted to be assign to \\n\", predis)\n",
    "#doing semisup kmean clustering\n",
    "    print(\"\\n now doing semi supervised k means clustering\")\n",
    "#data with some that arre not labeled \n",
    "    inputdata = np.array([\n",
    "        [1.0, 2.0],\n",
    "        [1.5, 1.8],\n",
    "        [5.0, 8.0],\n",
    "        [8.0, 8.0],\n",
    "        [1.0, 0.6],\n",
    "        [9.0, 11.0]\n",
    "    ])\n",
    "    labelz = np.array([0, 0, np.nan, np.nan, 0, np.nan])\n",
    "    semikmeanz = semikmclus(klusters=2, maxiterations=100, conver=1e-4)\n",
    "    cent, labelz = semikmeanz.train(inputdata, output=labelz)\n",
    "    print(\"the centroids in the data are \\n\", cent)\n",
    "    print(\"the datapoints are assigned to these clusters \\n\", labelz)\n",
    "#doing pca\n",
    "    print(\"\\n now doing pca\")\n",
    "    inputdata = np.array([\n",
    "        [1.0, 2.0],\n",
    "        [1.5, 1.8],\n",
    "        [5.0, 8.0],\n",
    "        [8.0, 8.0],\n",
    "        [1.0, 0.6],\n",
    "        [9.0, 11.0]\n",
    "    ])\n",
    "#define num of compotnents and train the data \n",
    "    numcom = 2 \n",
    "    doingpca = princompanaly(numcom=numcom)\n",
    "    doingpca.train(inputdata)\n",
    "#do rpedictions \n",
    "    print(\"the original data \\n\", inputdata)\n",
    "    tansfdata = doingpca.predict(inputdata)\n",
    "    print(\"data after the pca \\n\", tansfdata)\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
